{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFA from GEFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the efa_xray package (https://github.com/lmadaus/efa_xray.git), specifically the simple_gefs branch, to grab the \"latest\" GEFS ensemble forecast and update it with the most recent observations as they become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/aleutians/nobackup/lmadaus/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "# Some initial imports of libraries we will use\n",
    "from __future__ import print_function, division\n",
    "import cPickle\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below uses the Siphon libraries (https://github.com/Unidata/siphon) to download the requested subset of the latest GEFS ensemble.  Specifically, you give the function a list of the state variables you want and optionally the lat/lon bounds of a geographical area you want and it will download the corresponding GEFS forecast and format into an efa_xray ensemble state object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def state_from_gfse(statevars=['t2'], bounds=None):\n",
    "    \"\"\" \n",
    "    Build an xray state vector from GEFS forecasts \n",
    "    \n",
    "    Requires:\n",
    "    statevars -> A list of state variables to collect.  These must appear as keys \n",
    "                 in the \"vardict\" dictionary within this function.\n",
    "    bounds    -> A 4-member tuple of bounding lats and lons: (West, East, South, North)\n",
    "                 If this is None (default), will download entire spatial domain of ensemble\n",
    "   \n",
    "    \n",
    "    Returns:\n",
    "    An ensemble state object\n",
    "    ny -> Number of Y points in original state\n",
    "    nx -> Number of X points in original state\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Import the following items.  The siphon_gefs script is specific\n",
    "    # to the simple_gefs branch of code\n",
    "    from efa_xray.ensemble_class import Xray_Ensemble_State\n",
    "    from netCDF4 import Dataset, num2date\n",
    "    from datetime import datetime, timedelta\n",
    "    from numpy import reshape, zeros\n",
    "    import os\n",
    "    from siphon_gfse import get_gefs_ensemble\n",
    "    \n",
    "    \n",
    "    # Here we have a dictionary translatingsome generic names \n",
    "    # for possible state vector variables and translating to\n",
    "    # what they correspond to in the raw GEFS files\n",
    "    vardict = {'t2' : 'Temperature_height_above_ground_ens',\n",
    "               'prmsl' : 'Pressure_reduced_to_MSL_msl_ens',\n",
    "               'psfc' : 'Pressure_surface_ens',\n",
    "               }\n",
    "    \n",
    "    # Get the data from Unidata/Siphon\n",
    "    # This returns a netcdf-like object\n",
    "    raw_ensemble = get_gefs_ensemble([vardict[x] for x in statevars], bounds=bounds, writeout=False)\n",
    " \n",
    "\n",
    "    \n",
    "    # Now build the metadata to describe this ensemble\n",
    "    # Create an empty dictionary to hold metadata\n",
    "    meta = {}\n",
    "    \n",
    "    # Sometimes the time variable in the GEFS is listed as \"time2\"\n",
    "    # and at other times it's just \"time\".  Check to see what we\n",
    "    # have here\n",
    "    if 'time' in raw_ensemble.dimensions.keys():\n",
    "        timevar = 'time'\n",
    "    elif 'time1' in raw_ensemble.dimensions.keys():\n",
    "        timevar = 'time1'\n",
    "    elif 'time2' in raw_ensemble.dimensions.keys():\n",
    "        timevar = 'time2'\n",
    "    else:\n",
    "        print(\"Unable to find a time dimension in GEFS raw ensemble.\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # Get the dimensions of the state\n",
    "    nmems = len(raw_ensemble.dimensions['ens'])\n",
    "    ntimes = len(raw_ensemble.dimensions[timevar])\n",
    "    ny = len(raw_ensemble.dimensions['lat'])\n",
    "    nx = len(raw_ensemble.dimensions['lon'])\n",
    "    nvars = len(statevars)\n",
    "    \n",
    "    # Total number of geographic locations\n",
    "    nlocs = ny*nx\n",
    "    \n",
    "    # Allocate an empty array to hold the entire state vector\n",
    "    print(\"Allocating the state vector array...\")\n",
    "    state = zeros((nvars, ntimes, nlocs, nmems))\n",
    "    \n",
    "    # For metadata, need list of times and locations\n",
    "    # Use num2date to convert to datetime objects\n",
    "    times = raw_ensemble.variables[timevar]\n",
    "    valid_times = num2date(times[:], times.units)\n",
    "    \n",
    "    # Locations will be flattened out into a single dimension\n",
    "    # Location is every latitude and longitude in the flattened array\n",
    "    # First, need to expand the 1d latitude array into a full 2d\n",
    "    # array so we have latitude at each grid point\n",
    "    latl = np.array(raw_ensemble.variables['lat'][:])[:,None]*np.ones((ny,nx))\n",
    "    # Then flatten this 2d array to a 1d array\n",
    "    lats = list(latl.flatten())\n",
    "    \n",
    "    # Now do the same for the longitudes\n",
    "    lonl = np.array(raw_ensemble.variables['lon'][:])[None,:]*np.ones((ny,nx))\n",
    "    lons = list(lonl.flatten())\n",
    "    # Convert to negative lons\n",
    "    lons = [l-360.0 if l > 180.0 else l for l in lons]\n",
    "    \n",
    "    # Zip the latitudes and longitudes together.  This gives a lat-lon pair\n",
    "    # for each point.\n",
    "    locations = zip(lats,lons)\n",
    "    # Xray doesn't like tuples as coordinates, so need to make these strings\n",
    "    locations = ['{:7.4f},{:7.4f}'.format(l[0],l[1]) for l in locations]\n",
    "\n",
    "    # Compose the metadata\n",
    "    print(\"Building state metadata...\")\n",
    "    meta[(0,'var')] = statevars\n",
    "    meta[(1,'time')] = valid_times\n",
    "    meta[(2,'location')] = locations\n",
    "    meta[(3,'mem')] = [n+1 for n in xrange(nmems)]\n",
    "    # metadata is now done\n",
    "    \n",
    "    \n",
    "    # Now we can populate the state array\n",
    "    # Loop through each state variable requested\n",
    "    # and insert it into the empty array we allocated earlier\n",
    "    for var in statevars:\n",
    "        # Squeeze this variable to remove any singleton dimensions\n",
    "        field = np.squeeze(raw_ensemble.variables[vardict[var]][:])\n",
    "        # For surface pressure fields, put it in hPa\n",
    "        if var in ['prmsl','psfc']:\n",
    "            field = field / 100.\n",
    "        # Reshape this to be flattened in space (so it matches the location metadata)\n",
    "        field = np.reshape(field,(ntimes,nmems,ny*nx))\n",
    "        # Swap the last two axes so members dimension is last\n",
    "        # This makes most operations faster\n",
    "        field = np.swapaxes(field,1,2)\n",
    "        # Put this variable into the state array\n",
    "        state[statevars.index(var),:,:,:] = field\n",
    "    \n",
    "    # The following block of code is an example of reading the state\n",
    "    # from multiple existing netcdf files (one per member) instead of\n",
    "    # using siphon\n",
    "    \"\"\"\n",
    "    for mnum, mem in enumerate(memfiles):\n",
    "        # Point to the file\n",
    "        data = Dataset('/'.join((filedir,mem)),'r')\n",
    "        # If this is the first member, calculate how large the state\n",
    "        # array needs to be and allocate.  Also set up the metadata.\n",
    "        if mnum == 0:\n",
    "            nmems = len(memfiles)\n",
    "            ntimes = len(data.dimensions['time'])\n",
    "            nvars = len(statevars)\n",
    "            ny = len(data.dimensions['latitude'])\n",
    "            n3x = len(data.dimensions['longitude'])\n",
    "            nlocs = ny * nx\n",
    "            # Allocate the state array\n",
    "            print(\"Allocating the state vector array...\")\n",
    "            state = zeros((nvars,ntimes,nlocs,nmems))\n",
    "            \n",
    "            # For the metadata, need a list of times and locations\n",
    "            ftimes = [datetime(1970,1,1) + timedelta(seconds=d) for d in data.variables['time'][:]]\n",
    "            # Location is every latitude and longitude in the flattened array\n",
    "            latl = np.array(data.variables['latitude'][:])[:,None]*np.ones((ny,nx))\n",
    "            lats = list(latl.flatten())\n",
    "            \n",
    "            lonl = np.array(data.variables['longitude'][:])[None,:]*np.ones((ny,nx))\n",
    "            lons = list(lonl.flatten())\n",
    "            # Convert to negative lons\n",
    "            lons = [l-360.0 if l > 180.0 else l for l in lons]\n",
    "            # Zip these\n",
    "            locations = zip(lats,lons)\n",
    "            locations = ['{:3.4f},{:3.4f}'.format(l[0],l[1]) for l in locations]\n",
    "            \n",
    "            # Compose the metadata\n",
    "            print(\"Building state metadata...\")\n",
    "            meta[(0,'var')] = statevars\n",
    "            meta[(1,'time')] = ftimes\n",
    "            meta[(2,'location')] = locations\n",
    "            meta[(3,'mem')] = [n+1 for n in xrange(nmems)]\n",
    "        \n",
    "        # Now we can populate the state array\n",
    "        for var in statevars:\n",
    "            field = data.variables[vardict[var]][:]\n",
    "            if var in ['prmsl','psfc']:\n",
    "                field = field / 100.\n",
    "            # Reshape this to be flattened in space\n",
    "            field = reshape(field,(ntimes,ny*nx))\n",
    "            # Populate its component of the state array\n",
    "            state[statevars.index(var),:,:,mnum] = field\n",
    "        data.close()\n",
    "    \"\"\"\n",
    "    # Some cleanup\n",
    "    # Free up the memory held by the raw_ensemble object\n",
    "    del raw_ensemble\n",
    "    \n",
    "    # Print out the ensemble shape\n",
    "    print(\"State dimension sizes:\")\n",
    "    for name,dat in meta.items():\n",
    "        print(name,len(dat))\n",
    "    # Package this into an Xray_EnsembleSstate object knowing the state and metadata\n",
    "    statecls = Xray_Ensemble_State(state=state, meta=meta)\n",
    "    return statecls, ny, nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an Xray_Ensemble_State object? It uses the xray libraries (http://xray.readthedocs.org/en/stable/) to package the state as a multi-dimensional array and facilitate more intuitively accessing specific parts of that array.  It also adds several methods that can work on the state object to perform common tasks in ensemble data assimilation.  We can look at a list of these methods below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Xray_Ensemble_State in module efa_xray.ensemble_class:\n",
      "\n",
      "class Xray_Ensemble_State\n",
      " |  Define an ensemble state vector\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, state=None, meta=None, usevars=None, usedims=None)\n",
      " |      Initialize based on the input.  We are either given a\n",
      " |      netCDF4 \"Dataset\" object as \"state\" with a list of variables and dimensions\n",
      " |      to use OR we are given a numpy ndarray as a state with the variables and\n",
      " |      dimensions specified in the variable \"meta\"\n",
      " |  \n",
      " |  chunk_bounds(self, nChunks)\n",
      " |      Function to compute the bounding array locations when dividing up a\n",
      " |      state array.  Returns a dictionary.\n",
      " |  \n",
      " |  ensemble_mean(self)\n",
      " |      Returns the ensemble mean of the state as a DataArray\n",
      " |  \n",
      " |  ensemble_perts(self)\n",
      " |      Removes the ensemble mean and returns an Xray DataArray        of the perturbations from the ensemble mean\n",
      " |  \n",
      " |  ensemble_times(self)\n",
      " |      Return the values of the time dimension AS DATETIME OBJECTS\n",
      " |  \n",
      " |  locations(self)\n",
      " |      Return an array of latitude/longitude pairs for each location in the\n",
      " |      state\n",
      " |  \n",
      " |  num_locs(self)\n",
      " |      Returns number of locations\n",
      " |  \n",
      " |  num_mems(self)\n",
      " |      Returns number of ensemble members\n",
      " |  \n",
      " |  num_state(self)\n",
      " |      Returns length of state vector\n",
      " |  \n",
      " |  num_times(self)\n",
      " |      Returns number of times in the ensemble\n",
      " |  \n",
      " |  num_vars(self)\n",
      " |      Returns number of variables in the ensemble\n",
      " |  \n",
      " |  project_coordinates(self, m, ny, nx)\n",
      " |      Function to return projected coordinates given:\n",
      " |      m --> a Basemap instance defining the projection\n",
      " |      ny --> Number of y points\n",
      " |      nx --> Number of x points\n",
      " |      Returns:\n",
      " |          gy,gx --> The projected coordinate arrays\n",
      " |  \n",
      " |  reintegrate_state(self, state_chunks)\n",
      " |      Reintegrate the state vector from various chunks.  The opposite of\n",
      " |      split_state.  This will overwrite self.state\n",
      " |  \n",
      " |  shape(self)\n",
      " |      Returns the full shape of the DataArray\n",
      " |  \n",
      " |  split_state(self, nChunks)\n",
      " |      Function to split the state Xray object into nChunks number of\n",
      " |      smaller Xray objects for multiprocessing.  Returns a dictionary of state\n",
      " |      chunks.  This dictionary may be re-fed into the master state using the\n",
      " |      function \"reintegrate_state\" which will overwrite the master state Xray\n",
      " |      object with the separate parts\n",
      " |  \n",
      " |  state_to_array(self)\n",
      " |      Returns an array of the values in a shape of \n",
      " |      Nstate x Nmems\n",
      " |  \n",
      " |  update_state_from_array(self, instate)\n",
      " |      Takes an Nstate x Nmems ndarray and rewrites the state accordingly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from efa_xray.ensemble_class import Xray_Ensemble_State\n",
    "help(Xray_Ensemble_State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually build a state using the latest GEFS forecast.  We do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF3_CLASSIC data model, file format NETCDF3):\n",
      "    Originating_or_generating_Center: US National Weather Service, National Centres for Environmental Prediction (NCEP)\n",
      "    Originating_or_generating_Subcenter: NCEP Ensemble Products\n",
      "    GRIB_table_version: 2,1\n",
      "    Type_of_generating_process: Ensemble forecast\n",
      "    Analysis_or_forecast_generating_process_identifier_defined_by_originating_centre: Global Ensemble Forecast System (GEFS)\n",
      "    Conventions: CF-1.6\n",
      "    history: Read using CDM IOSP GribCollection v3\n",
      "    featureType: GRID\n",
      "    History: Translated to CF-1.0 Conventions by Netcdf-Java CDM (CFGridWriter2)\n",
      "Original Dataset = /data/ldm/pub/native/grid/NCEP/GEFS/Global_1p0deg_Ensemble/member/GEFS_Global_1p0deg_Ensemble_20160309_1800.grib2.ncx3#LatLon_181X360-p5S-180p0E; Translation Date = 2016-03-10T00:40:22.901Z\n",
      "    geospatial_lat_min: 24.0\n",
      "    geospatial_lat_max: 49.0\n",
      "    geospatial_lon_min: -125.0\n",
      "    geospatial_lon_max: -67.0\n",
      "    dimensions(sizes): time(10), ens(21), height_above_ground(1), lat(26), lon(59)\n",
      "    variables(dimensions): float32 \u001b[4mTemperature_height_above_ground_ens\u001b[0m(time,ens,height_above_ground,lat,lon), float64 \u001b[4mtime\u001b[0m(time), int32 \u001b[4mens\u001b[0m(ens), float32 \u001b[4mheight_above_ground\u001b[0m(height_above_ground), float32 \u001b[4mlat\u001b[0m(lat), float32 \u001b[4mlon\u001b[0m(lon)\n",
      "    groups: \n",
      "\n",
      "Allocating the state vector array...\n",
      "Building state metadata...\n",
      "State dimension sizes:\n",
      "(3, 'mem') 21\n",
      "(2, 'location') 1534\n",
      "(0, 'var') 1\n",
      "(1, 'time') 10\n",
      "Forecast times in state: [datetime.datetime(2016, 3, 9, 18, 0), datetime.datetime(2016, 3, 10, 0, 0), datetime.datetime(2016, 3, 10, 6, 0), datetime.datetime(2016, 3, 10, 12, 0), datetime.datetime(2016, 3, 10, 18, 0), datetime.datetime(2016, 3, 11, 0, 0), datetime.datetime(2016, 3, 11, 6, 0), datetime.datetime(2016, 3, 11, 12, 0), datetime.datetime(2016, 3, 11, 18, 0), datetime.datetime(2016, 3, 12, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of state variables.  Here let's just grab 2m temperature\n",
    "statevars = ['t2']\n",
    "# Only focus on CONUS--these are W,E,S,N lon-lat bounds\n",
    "bounds=(-124.9,-66.8,24.3,49.4)\n",
    "# For now, we are going to have observations of the same variables\n",
    "# as in the state (instead of a subset)\n",
    "obtypes = statevars\n",
    "\n",
    "# Download the latest GFSE ensemble and package it into an Xray_Ensemble_State object\n",
    "# using the above function\n",
    "state_vect, ny, nx = state_from_gfse(statevars=statevars, bounds=bounds)\n",
    "\n",
    "# We can use the methods of this object to tell us about our state\n",
    "ftimes = state_vect.ensemble_times()\n",
    "print(\"Forecast times in state:\", ftimes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a siphon utility for getting observations.  The only problem is the observations on siphon seem to be out of date (they are several days behind).  So we are going to use a clunky function that parses data from locally-stored files at the UW and returns a list of Observation objects from the metar observations at times that overlap with our ensemble state times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mesowest_observations import get_mesowest_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's in an Observation object? We can look at its methods with the help function like we did with the Xray_Ensemble_State class.  The Observation object also stores the value, type, time, error, and location values for the ob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Observation in module efa_xray.observation_class:\n",
      "\n",
      "class Observation\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  H(self, state)\n",
      " |      Given an ensemble state class, compute an H\n",
      " |  \n",
      " |  H_Xb(self, state)\n",
      " |      Return the ensemble (state) estimate of the ob\n",
      " |  \n",
      " |  __init__(self, value=None, obtype=None, time=None, error=None, location=None, prior_mean=None, post_mean=None, prior_var=None, post_var=None, assimilate_this=False, description=None, localize_radius=None)\n",
      " |  \n",
      " |  distance_to_state(self, state)\n",
      " |      Return the distance from this ob to all locations in the state\n",
      " |      vector (in km)\n",
      " |  \n",
      " |  localize(self, state, type='GC', full_state=False)\n",
      " |      Given a state vector object, assume location is in lat/lon and compute a\n",
      " |      Gaspari-Cohn weighting function with the specified halfwidth (in km)\n",
      " |  \n",
      " |  map_localization(self, state, m, ny, nx, type='GC')\n",
      " |      Function to map localization radius \n",
      " |      Requires:\n",
      " |          state --> The state vector we are using\n",
      " |          m     --> A basemap instance for projecting the map\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from efa_xray.observation_class import Observation\n",
    "help(Observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use that clunky function above to get the METAR observations that overlap with our ensemble times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obs found: 1323\n"
     ]
    }
   ],
   "source": [
    "# Now we grab the observations.  Note we've specified the sites we want in siteids,\n",
    "# the observation types we want in obtypes (which is the same as statevars, here t2),\n",
    "# and the times to get in \"use_times\" (which is the same as the times in the ensemble)\n",
    "observations = []\n",
    "for time in ftimes:\n",
    "    if time <= datetime.utcnow():\n",
    "        newobs = get_mesowest_obs(time, bounds=bounds)\n",
    "        observations += newobs\n",
    "print('Number of obs found:', len(observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an Xray_Ensemble_State and a list of observation objects. There are a few fun methods build into the observations and the ensemble state classes we can leverage.  Let's first define a background map we can plot on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This builds a python basemap object of a mercator projection\n",
    "# using the lat-lon bounds of our domain\n",
    "\n",
    "m = Basemap(projection='merc', resolution = 'l',\\\n",
    "            llcrnrlon=bounds[0], urcrnrlon=bounds[1],\n",
    "            llcrnrlat=bounds[2], urcrnrlat=bounds[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a built-in method to look at how localization works.  Each Observation object as a \"map_localization\" method, where you can feed the observation a state vector, a map to plot on, and the 2-d dimensions of the map (ny and nx from before) and it will make a map of localization weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This grabs the observation at location [1] in the list and plots its localization weights\n",
    "observations[1].map_localization(state_vect, m, ny, nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change individual facets for each observation.  For instance, the default localization radius is 1000km.  You could change this ob's localization radius to something like 2000 km..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observations[1].localize_radius = 2000.0\n",
    "observations[1].map_localization(state_vect, m, ny, nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, each observation has an attribute \"assimilate_this\" that needs to be set to True for the enkf to assimilate the ob.  We can loop through the observations and set this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll put the localization radius for that ob back where it was\n",
    "observations[1].localize_radius = 1000.0\n",
    "# Loop through the observations\n",
    "for ob in observations:\n",
    "    # Make all obs False\n",
    "    ob.assimilate_this = False\n",
    "    # However, if the ob's time is between 12Z and 18Z, assimilate it\n",
    "    if (ob.time.hour <= 18) and (ob.time.hour >= 12):\n",
    "        ob.assimilate_this = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the obs, we have the state, we can now assimilate them.  Import the enkf_update utility and use the \"update\" method to do the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15340,) 15340\n",
      "(15340, 21)\n"
     ]
    }
   ],
   "source": [
    "# Import the enkf_update method\n",
    "import efa_xray.enkf_update\n",
    "reload(efa_xray.enkf_update)\n",
    "\n",
    "# We feed the enkf the state_vect and observations, specify localization to be Gaspari-Cohn (GC)\n",
    "# At this point, LEAVE NPROC SET TO 1.  This is supposed to allow you to do multiprocessing in the\n",
    "# future, but it doesn't work quite right yet.\n",
    "# The function returns a posterior state and observations\n",
    "# The posterior state is another Xray_Ensemble_State object\n",
    "post_state, post_obs = efa_xray.enkf_update.update(state_vect, observations, loc='GC',nproc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're just going to loop through a subset of the observations and print statistics about the assimilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ob in observations[0:100:10]:\n",
    "    prior_mean = np.mean(ob.H_Xb(state_vect))\n",
    "    post_mean = np.mean(ob.H_Xb(post_state))\n",
    "    prior_error = abs(prior_mean - ob.value)\n",
    "    post_error = abs(post_mean - ob.value)\n",
    "    print(ob.description, ob.time, ob.obtype, ob.value, prior_mean, ob.assimilated, post_mean, post_error - prior_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we're going to plot the increments to the forecast at a variety of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to plot every other time in the forecast (every 12 hours)\n",
    "timelist = ftimes[::2]\n",
    "# Figure out how many panels we need for our figure\n",
    "ntimes = len(timelist)\n",
    "nrows = ntimes / 2 + 1\n",
    "\n",
    "# Make the plot\n",
    "plt.figure(figsize=(12,6))\n",
    "# Loop through each time we identified above\n",
    "for tnum, t in enumerate(timelist):\n",
    "    # New subplot\n",
    "    plt.subplot(2,nrows,tnum+1)\n",
    "    # This line takes the ensemble mean from the entire prior state and finds all values where\n",
    "    # the time is equal to this time and the variable t2.  The ability to do\n",
    "    # this in one line is why we use the Xray libraries\n",
    "    prior_mean = state_vect.ensemble_mean().loc[dict(time=t,var='t2')].values\n",
    "    # Same for the posterior (updated) state\n",
    "    post_mean = post_state.ensemble_mean().loc[dict(time=t,var='t2')].values\n",
    "    \n",
    "    # Remember way back at the beginning our locations got flattened out into\n",
    "    # 1d arrays.  Here we reshape them to 2d arrays again.\n",
    "    prior_mean = np.reshape(prior_mean,(ny,nx))\n",
    "    post_mean = np.reshape(post_mean, (ny,nx))\n",
    "    # Compute the increment by subtracting the updated mean from the prior mean\n",
    "    increment = np.subtract(post_mean, prior_mean)\n",
    "    # Use our baemap object to project into plotting coordinates\n",
    "    x,y = state_vect.project_coordinates(m,ny,nx)\n",
    "    # Plot the map\n",
    "    incplot = plt.pcolormesh(x,y,increment,vmin=-2.0,vmax=2.0,cmap = matplotlib.cm.RdBu_r)\n",
    "    # Draw the coastlines and boundaries\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    m.drawstates()\n",
    "    # Title is the valid date\n",
    "    plt.title(t.strftime('%d/%HZ'))\n",
    "    #if tnum == 0:\n",
    "    #    plt.colorbar(incplot)\n",
    "# This makes a master title\n",
    "plt.suptitle('Increment of T2 (assim 12Z+18Z obs)')\n",
    "# Cleans up the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
